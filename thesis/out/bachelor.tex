\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Jonathan Werner},
            pdftitle={Bachelorarbeit - Better accuracy of automatic lecture transcriptions by using context information from slide contents},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

\title{Bachelorarbeit - Better accuracy of automatic lecture transcriptions by
using context information from slide contents}
\author{Jonathan Werner}
\date{}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[all]{hypcap}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\begin{document}
\maketitle

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{3}
\tableofcontents
}
\newpage

\section*{Introduction}\label{introduction}
\addcontentsline{toc}{section}{Introduction}

Scannability is crucial for academic research: you have to be able to
quickly evaluate the usefulness of a given resource by skimming the
content and looking for the parts that are specifically relevant to the
task at hand.

The medium in which those resources are available is very centered on
textual representation. Spoken content, hereinafter called \emph{speech
media} (audio- or audiovisual media that mainly consists of spoken
language) doesn't make it possible to scan its contents. You are
``stabbing in the dark'' when looking for something specific in a medium
like this and have to consume it like a linear narrative.

This means that although lectures and conference talks are a central
element to science they are much more challenging and tedious to use for
research work.

Being able to a) efficiently search and b) look at the temporal
distribution of important keywords in a visually dense way would elevate
the usefulness of speech media in the scientific context immensely.

One approach to accomplish those goals is utilizing Automatic Speech
Recognition (ASR) to transcribe speech to text and also get timing
information for the recognized words. This makes it possible to derive
information about the density of given words at a given point of time in
the talk, which in turn allows to compute word occurence density maxima.
This opens up possibilities for compact visual representation of the
interesting keywords, thus allowing the user to scan.

The main challenge when using ASR for this task is the recognition
accuracy of technical terms. Most of them are not included in the
language models that are available as those are broad and generic so as
to optimize for accuracy over a wide topic spectrum. But when they are
not included into the language model they have a very small chance to be
correctly recognized at all.

So the usefulness of applying ASR with a generic language model to the
problem is very small, as the intersection of interesting keywords with
those technical terms that can not be recognized is very big.

The central goal of this thesis is to explore an approach to overcome
this problem. This approach consists of using words from lecture slides
or other notes to generate a lecture-specific language model. This is
then interpolated with a generic language model and being compared to
the `baseline' accuracy of the generic model.

\pagebreak

\subsection*{Structure of this thesis}\label{structure-of-this-thesis}
\addcontentsline{toc}{subsection}{Structure of this thesis}

The structure of this thesis is laid out as follows:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  \textbf{Research questions}

  I will state the research questions.
\item
  \textbf{Scientific Background}

  \begin{enumerate}
  \item
    I will start by giving an overview over the state of the art of ASR
    and the most prevalent approaches.
  \item
    I will explain the \emph{concepts} which are fundamental for the
    understanding of speech recognition.
  \item
    I will then examine the \emph{scientific work} that has been done on
    applying ASR to the problem of lectures transcriptions.
  \item
    Finally i will summarize the \emph{metrics} that have been used to
    assess the quality of the improvements in different approaches.
  \end{enumerate}
\item
  \textbf{Motivation}

  Here i will motivate why it is necessary to improve on the baseline
  performance of ASR in our context.

  I will talk about the role of keywords and technical terms and why
  they are not being detected and how that diminishes the usefulness of
  ASR for the purposes of scannability.
\item
  \textbf{Test data}

  I will use the openly available \emph{Open Yale Courses} (\emph{Open
  Yale Courses Website}, n.d.), which provide a diverse selection of
  audio and video recordings of university lectures at Yale,
  additionally supplying quality manual transcriptions and course notes
  or slides.

  I will present the chosen courses, their selection criteria and
  discuss the range of types of lecture material.
\item
  \textbf{The LM-Interpolation approach}

  \begin{enumerate}
  \item
    \textbf{Technical basis}

    I will introduce the open source speech recognition framework
    \emph{Sphinx 4}. This is the software that is used for performing
    the actual recognition.
  \item
    \textbf{Process overview}

    I will then give a overview of the design and architecture of our
    approach.
  \item
    \textbf{Implementation}

    Finally i will describe the technical implementation by which the
    lecture material is compiled into a specialized language model and
    recognition is performed using a \emph{interpolated} language model.
  \end{enumerate}
\item
  \textbf{Analysis}

  \begin{enumerate}
  \item
    \textbf{Methods}

    I will discuss how to analyze the results and develop metrics that
    assess how well the given goals are met with our approach. 
  \item
    \textbf{Analysis}

    I will then perform quantitative analysis on our test dataset with
    the metrics we developed before.
  \item
    \textbf{Discussion, Finding and Conclusions}

    I will discuss the findings and draw conclusions from the
    quantitative analysis concerning the effectiveness of our approach.
  \end{enumerate}
\item
  \textbf{Visualization for Scannability}

  I will present a prototype visualization method that uses the results
  from our approach to present a condensed representation of the keyword
  content from lectures with the goal of providing a quick, interactive
  way to search and scan speech media.
\item
  \textbf{Improvements, Open Ends}

  I will discuss possible improvements and open ends that were out of
  the scope of this thesis but would be interesting to explore further.
\item
  \textbf{Summary}

  I will end by summarizing the goals, the proposed approach, the design
  and implementation, the analysis and the results.
\end{enumerate}

\pagebreak

\section{Research questions}\label{research-questions}

The central research questions i want to investigate in this thesis can
be formulated as follows:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  When we apply ASR to university lectures, what is the advantage of
  using an approach that consists of creating a lecture-specific
  language model and interpolating it with a generic language model,
  given that we are interested in improving the recognition accuracy of
  \emph{interesting keywords} for the sake of searchability and
  scannability?
\item
  What metric is useful for quantifying this advantage?
\end{enumerate}

A secondary question is: How can we \emph{use} the results from our
approach to provide graphical interfaces for improving the users ability
to search and scan the given speech medium?

The exploration of this question will not be the center of this thesis,
but it will provide practical motivation for the results that the
exploration.

\section{Background}\label{background}

\subsection{The field of Automatic Speech
Recognition}\label{the-field-of-automatic-speech-recognition}

Automatic Speech Recognition (ASR) can be defined as the process by
which a computer maps an acoustic speech signal to text
(\emph{comp.speech Frequently Asked Questions}, n.d.).

Rabiner \& Juang (\hyperref[ref-rabiner]{1993}) date the first research
on ASR back to the early 1950s, when Bell Labs built a system for
single-speaker digit recognition. Since then the field has seen three
major approaches, which Marquard (\hyperref[ref-marquard]{2012})
summarizes as follows:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \emph{acoustic-phonetic approach} aimed to identify features of
  speech such as vowels directly through their acoustic properties, and
  from there build up words based on their constituent phonetic
  elements.
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The \emph{statistical pattern-recognition approach} measures features
  of the acoustic signal, and compares these to existing patterns
  established from a range of reference sources to produce similarity
  scores which may be used to establish the best match.
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \emph{Artificial intelligence (AI) approaches} have been used to
  integrate different types of knowledge sources (such as acoustic,
  lexical, syntactic, semantic and pragmatic knowledge) to influence the
  output from a pattern-recognition system to select the most likely
  match.
\end{enumerate}
\end{quote}

The most prevalent approach today is the \emph{statistical
pattern-recognition approach}, as it produces results with much higher
accuracy compared to the acoustic-phonetic approach. The use of Hidden
Markov Models (HMM) has been playing a key role in this approach, as it
allows recognizers to use a statistical model of a given pattern rather
than a fixed representation.

In the last years there has been a resurgence of AI approaches,
specifically \emph{deep learning approaches} (Hinton et al.,
\hyperref[ref-hinton2012deep]{2012}). The ASR paradigm we will use for
this thesis will be limited to the former, however.

\subsection{Dimensions of speech
recognition}\label{dimensions-of-speech-recognition}

There are three dimensions which serve to classify different
applications of speech recognition (\emph{comp.speech Frequently Asked
Questions}, n.d., Marquard (\hyperref[ref-marquard]{2012})):

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  \textbf{Dependent vs.~independent}. Dependent recognition systems are
  developed to be used by one speaker, whereas independent systems are
  developed to be used by \emph{any} speaker of a particular type, i.e
  North-American speakers. \textbf{Adaptive} systems lie between those
  poles, they are able to adapt to a particular speaker through
  training.
\item
  \textbf{Small vs.~large vocabulary}. Small vocabularies contain only
  up to a few hundred words and might be modeled by an explicit grammar,
  whereas large vocabularies contain tens of thousands of words so as to
  be able to model general purpose spoken language over a variety of
  domains.
\item
  \textbf{Continuous vs.~isolated speech}. Isolated speech consists of
  single words that are spoken with pauses in between them, whereas
  continuous speech consists of words that are spoken in a connected
  way. Continuous speech is significantly more difficult to recognize,
  as it is a) more difficult to find the start and end of words and b)
  the pronunciation of words changes in relation to their surrounding
  words.
\end{enumerate}

With those three dimensions we can for example classify the application
areas command and control systems, dictation and lecture transcription
(Marquard, \hyperref[ref-marquard]{2012}):

\begin{longtable}[c]{@{}llll@{}}
\caption{Three application areas}\tabularnewline
\toprule
Application & Speaker & Vocabulary & Duration\tabularnewline
\midrule
\endfirsthead
\toprule
Application & Speaker & Vocabulary & Duration\tabularnewline
\midrule
\endhead
Dictation & Dependent & Large & Connected\tabularnewline
Command and control system & Independent & Small &
Isolated\tabularnewline
Lecture transcription & Independent & Large & Connected\tabularnewline
\bottomrule
\end{longtable}

The task of automatic lecture transcriptions can thus be characterized
as speaker-independent (SI) large continuous speech recognition (LVCSR).

\subsection{Concepts}\label{concepts}

Speech recognition in the \emph{statistical pattern-recognition
approach} paradigm has three major concepts necessary for its
understanding:

\begin{itemize}
\tightlist
\item
  phonemes and phonetic dictionaries
\item
  acoustic models (AM)
\item
  language models (LM)
\end{itemize}

\subsubsection{Phonemes}\label{phonemes}

A \emph{phoneme} is ``the smallest contrastive linguistic unit which may
bring about a change of meaning'' (Cruttenden,
\hyperref[ref-cruttenden2014gimson]{2014}, p. 43). They are the smallest
unit of sound in speech which are combined to form words. The word
\emph{sun} for example can be represented by the phonemes \texttt{/s/},
\texttt{/u/} and \texttt{/n/}; the word \emph{table} by \texttt{/t/},
\texttt{/a/} and \texttt{/bl/}.

A language together with a specific accent can be described by a set of
phonemes that it consists of. Figure \ref{phonemic-chart} uses symbols
from the International Phonetic Alphabet (IPA) to display the 44
phonemes that are being used in Received Pronunciation (RP), which is
regarded as the ``standard accent'' in the south of the United Kingdom
(Stevenson \& Waite, \hyperref[ref-stevenson2011concise]{2011}).

\begin{figure}[htbp]
\centering
\includegraphics{images/phonemes_50.jpg}
\caption{Phonemic Chart representing 44 phonemes used in RP British
English\label{phonemic-chart}}
\end{figure}

To be able to use phonemes in software an ASCII representation is more
suitable. The standard for General American English is the
\emph{Arpabet}. Here each phoneme is mapped to one or two capital
letters. The digits \texttt{0}, \texttt{1} and \texttt{2} signify stress
markers: no stress, primary and secondary stress respectively. A
comparison of the IPA format and the arphabet format can be seen in
Figure \ref{arpabet}, an excerpt that just shows the
\emph{monophthongs}.\footnote{pure vowel sounds with relatively fixed
  articulation at the start and the end that don't glide towards a new
  position of articulation}

\begin{figure}[htbp]
\centering
\includegraphics{images/arpabet.png}
\caption{Excerpt from the Arpabet (\emph{English Wikipedia Arpabet
article}, 2015 (accessed 22.8.15)) \label{arpabet}}
\end{figure}

\subsubsection{Phonetic dictionaries}\label{phonetic-dictionaries}

Phonetic dictionaries map words to one or multiple versions of phoneme
sequences.

A phonetic representation of a word is specified manually from the
knowledge how written words \emph{actually sound} when spoken.

An excerpt from the dictionary \texttt{cmudict-en-us.dict} (\emph{CMU
EN-US Pronouncing Dictionary (cmudict-en-us.dict)},
\hyperref[ref-cmuDict]{2015}) looks like this:

\begin{verbatim}
...
abdollah AE B D AA L AH
abdomen AE B D OW M AH N
abdomen(2) AE B D AH M AH N
abdominal AE B D AA M AH N AH L
abdominal(2) AH B D AA M AH N AH L
...
\end{verbatim}

The dictionary has 133.425 entries. In the general case only words that
are in the phonetic dictionary being used can be recognized during
speech recognition. \emph{Grapheme\footnote{``The smallest unit used in
  describing the writing system of a language'' Florian
  (\hyperref[ref-florian1996blackwell]{1996}), p.174}-to-Phoneme
converters} (G2P) however make it possible to get phoneme sequence
hypotheses for arbitrary words (that meaning arbitrary sequences of
graphemes). While those results are on average less accurate than
manually created variants, they play a vital role in texts with many
technical terms as those are often not part of phonetic dictionaries.

\subsubsection{Acoustic models}\label{acoustic-models}

An acoustic model (AM) describes the relation between an audio signal
and the probability that this signal represents a given phoneme.

Acoustic models are created by \emph{training} them on a \emph{corpus}
of audio recordings and matching transcripts. When being used in the
context of speaker-independent recognition, those models are trained
with a variety of speakers that represent a broad spectrum of the
language/accent that the acoustic model should represent.

During the \emph{decoding} phase the acoustic model and a phonetic
dictionary are used to match sequences of small audio ``slices'' to
possible phonemes and those phonemes to possible word sequence
hypotheses.

However, acoustic models alone are not sufficient for speech recognition
as they don't have the ``higher-level'' linguistic information necessary
to for example decide between homonyms and similar-sounding phrases such
as ``wreck a nice beach'' and ``recognize speech'' (Marquard,
\hyperref[ref-marquard]{2012}, p. 11). This information finally is
provided by \emph{language models}.

\subsubsection{Language Models}\label{language-models}

Language models (LM) guide and constrain the search process a speech
recognition system performs by assigning probabilities to sequences of
words. They are trained by applying statistical methods on a text
corpus. Analogous to acoustic models, generic language models use huge
text corpora with a broad variety of topics. It is however possible to
train language models on small and specialized text corpora, which is
the central technical foundation for the approach discussed in this
thesis.

The most commonly used form of language models are \emph{n-gram language
models}. In the context of a language model a \emph{n-gram} is a
sequence of \emph{n} words. 1-grams are called \emph{unigrams}, 2-grams
are called \emph{bigrams} and 3-grams are called \emph{trigrams}. A
\emph{n-gram language model} maps a set of \emph{n-grams} to
probabilities that they occur in a given piece of text.

A key idea in modelling language like this is the \emph{independence
assumption}, which says that the probability of a given word is only
dependent on the last \emph{n} - 1 words. This assumption significantly
decreases the statistical complexity and makes it thus computationally
feasible.

N-gram language models don't need to be constrained to one type of
n-gram. The \emph{Generic US English Generic Language Model}
(\emph{CMUSphinx US English Generic Language Model
(cmusphinx-5.0-en-us.lm)}, \hyperref[ref-cmuLm]{2015}) from CMUSphinx we
will use as the baseline for our approach for example consists of 1-, 2,
and 3-grams.

A toy example of a language model with 1- and 2-grams when represented
in \emph{ARPA}-format (as used by CMUSphinx) looks like follows
(\emph{CMUSphinx ARPA Language models}, 2015 (accessed 23.8.15)):

\begin{verbatim}
\data\
ngram 1=7
ngram 2=7

\1-grams:
-1.0000 <UNK>   -0.2553
-98.9366 <s>     -0.3064
-1.0000 </s>     0.0000
-0.6990 wood     -0.2553
-0.6990 cindy   -0.2553
-0.6990 pittsburgh      -0.2553
-0.6990 jean     -0.1973

\2-grams:
-0.2553 <UNK> wood
-0.2553 <s> <UNK>
-0.2553 wood pittsburgh
-0.2553 cindy jean
-0.2553 pittsburgh cindy
-0.5563 jean </s>
-0.5563 jean wood

\end\
\end{verbatim}

Here the first number in a row is the probability of the given n-gram in
\(log_{10}\) format. This means that the unigram \emph{wood} has a
probability of \(10^{-0.6990} \approx 0.2 = 20\%\) and the probability
of the words ``wood pittsburg'' occuring in sequence is
\(10^{-0.2553} \approx 0.55 = 55\%\) .

The optional third numeric column in a row is called \emph{backoff
weight}. Backoff weights make it possible to calculate n-grams that are
not listed by applying the formula

\begin{verbatim}
P( word_N | word_{N-1}, word_{N-2}, ...., word_1 ) =
P( word_N | word_{N-1}, word_{N-2}, ...., word_2 ) *
  backoff-weight( word_{N-1} | word_{N-2}, ...., word_1 )
\end{verbatim}

With the side condition that missing entries for
\texttt{word\_\{N-1\}\ \textbar{}\ word\_\{N-2\},\ ....,\ word\_1} are
replaced by \(1.0\).

So if the text to be recognized would contain the sequence ``wood
cindy'', which does not appear as a bigram in the LM, the probability
for this bigram could be calculated by
\texttt{P(wood\textbar{}cindy)\ =\ P(wood)\ *\ BWt(cindy)}.

Finally, the overall probability of a sentence with the words
\(w_1,...,w_n\) can be approximated as follows:

\[P(w_1,...,w_n) = \prod_{n=1}^m P(w_i \mid w_1,...w_{i-1})\]

An example approximation with a bigram model for the sentence ``I saw
the red house'' (\emph{English Wikipedia Language Model article}, 2015
(accessed 23.8.15)) represented as \(P(\text{I, saw, the, red, house})\)
would look like \[
  P(\text{I} \mid \langle s \rangle) \times
  P(\text{saw} \mid \text{I}) \times
  P(\text{the} \mid \text{saw}) \times
  P(\text{red} \mid \text{the}) \times
  P(\text{house} \mid \text{red}) \times
  P(\langle s \rangle \mid \text{house})
\]

\subsection{Work done on ASR for lecture
transcription}\label{work-done-on-asr-for-lecture-transcription}

I will now give an overview over the scientific work done on lecture
transcription, using Marquard (\hyperref[ref-marquard]{2012}) as a
guiding reference.

The research for speech recognition on lectures can be partitioned into
three general approaches: generalization approaches, specialization
approaches and approaches involving the user for manual correction and
improvements.

\subsubsection{Generalization
approaches}\label{generalization-approaches}

Generalization approaches try to create models that capture common
characteristics of lectures. Those characteristics include highly
spontaneous presentation style and ``strong coarticulation effects,
non-grammatical constructions, hesitations, repetitions, and filled
pauses'' (Yamazaki, Iwano, Shinoda, Furui, \& Yokota,
\hyperref[ref-yamazaki]{2007}). Glass, Hazen, Hetherington, \& Wang
(\hyperref[ref-glass]{2004}) note the ``colloquial nature'' of lectures
as well as the ``poor planning at the sentence level {[}and{]} higher
structural levels''.

The generalization approach has been applied on the acoustic model
level: Cettolo, Brugnara, \& Federico (\hyperref[ref-cettolo]{2004})
have examined adapting a generic acoustic model to account for
spontaneous speech phenomena (``filler sounds'').

While the a subfield of ASR called ``speaker diarization'' tries to
account for the interactivity between lecturers and students by
identifying multiple speakers, most research treats lectures as single
speaker events with the audience as background noise.

Generalization approaches at the language model level try to model
common linguistic traits of the lecture genre (this can be called the
``macro level''). Kato, Nanjo, \& Kawahara
(\hyperref[ref-kato2000]{2000}) investigate topic-independent language
modeling by creating a large corpus of text from lecture transcripts and
panel discussions and then removing topic-specific keywords.\footnote{In
  a second step they combine this generalization technique with a
  specialization technique by adapting the resulting LM with a
  lecture-specific language model by using preprint papers of a given
  lecture.}

\subsubsection{Specialization
approaches}\label{specialization-approaches}

Specialization approaches try to use context specific to a single
lecture (``meso level'') or parts of a single lecture (``micro
level''\footnote{The three levels are taken from Marquard
  (\hyperref[ref-marquard]{2012}).}).

Methods used for creating LMs from context information can be
categorized into two approaches: direct usage of lecture slides and
notes for the creation of LMs versus usage of ``derived'' data from
those materials. Deriving data by using keywords found in slides, using
them as web search query terms and using the found documents as the
basis for LM creation is explored in Munteanu, Penn, \& Baecker
(\hyperref[ref-munteanu]{2007}), Kawahara, Nemoto, \& Akita
(\hyperref[ref-kawahara08]{2008}) and Marquard
(\hyperref[ref-marquard]{2012}).

Using the whole text from lecture slides has been explored by Yamazaki
et al. (\hyperref[ref-yamazaki]{2007}). They compare the \emph{meso
level} with the \emph{micro level} by dynamically adapting the LM for
the speech corresponding to a particular slide. Kawahara et al.
(\hyperref[ref-kawahara08]{2008}) also examine dynamic local
slide-by-slide adaption and compare it to global topic adaption using
Probabilistic Latent Semantic Analysis (PLSA)\footnote{Latent Semantic
  Analysis is an approach to document comparison and retrieval which
  relies on a numeric analysis of word frequency and proximity. } and
web text collection, concluding that the last performs worse then the
former because of a worse orientation to topic words. .

\subsection{Metrics}\label{metrics}

\section{Test data}\label{test-data}

The test data i'll use for evaluating our approach will be from
(\emph{Open Yale Courses Website}, n.d.). Lütkebohle
(\hyperref[ref-bworld]{2008})

\section{The LM-Interpolation
approach}\label{the-lm-interpolation-approach}

\section{Analysis}\label{analysis}

\newpage

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hyperdef{}{ref-cettolo}{\label{ref-cettolo}}
Cettolo, M., Brugnara, F., \& Federico, M. (2004). Advances in the
automatic transcription of lectures. In \emph{Acoustics, speech, and
signal processing, 2004. proceedings.(ICASSP'04). iEEE international
conference on} (Vol. 1, pp. I--769). IEEE.

\hyperdef{}{ref-cmuDict}{\label{ref-cmuDict}}
\emph{CMU EN-US Pronouncing Dictionary (cmudict-en-us.dict)}. (2015).
\url{https://github.com/cmusphinx/sphinx4/blob/master/sphinx4-data/src/main/resources/edu/cmu/sphinx/models/en-us/cmudict-en-us.dict}.

\hyperdef{}{ref-cmuArpa}{\label{ref-cmuArpa}}
\emph{CMUSphinx ARPA Language models}. (2015 (accessed 23.8.15)).
\url{http://cmusphinx.sourceforge.net/wiki/sphinx4:standardgrammarformats}.

\hyperdef{}{ref-cmuLm}{\label{ref-cmuLm}}
\emph{CMUSphinx US English Generic Language Model
(cmusphinx-5.0-en-us.lm)}. (2015).
\url{http://sourceforge.net/projects/cmusphinx/files/Acoustic\%20and\%20Language\%20Models/US\%20English\%20Generic\%20Language\%20Model/}.

\hyperdef{}{ref-cmufaq}{\label{ref-cmufaq}}
\emph{comp.speech Frequently Asked Questions}. (n.d.).
\url{http://www.speech.cs.cmu.edu/comp.speech/Section6/Q6.1.html}.

\hyperdef{}{ref-cruttenden2014gimson}{\label{ref-cruttenden2014gimson}}
Cruttenden, A. (2014). \emph{Gimson's pronunciation of English}.
Routledge.

\hyperdef{}{ref-wikiArpabet}{\label{ref-wikiArpabet}}
\emph{English Wikipedia Arpabet article}. (2015 (accessed 22.8.15)).
\url{https://en.wikipedia.org/wiki/Arpabet}.

\hyperdef{}{ref-wikiLM}{\label{ref-wikiLM}}
\emph{English Wikipedia Language Model article}. (2015 (accessed
23.8.15)). \url{https://en.wikipedia.org/wiki/Language_model}.

\hyperdef{}{ref-florian1996blackwell}{\label{ref-florian1996blackwell}}
Florian, C. (1996). The Blackwell encyclopedia of writing systems.
\emph{Oxford: Blackwell}.

\hyperdef{}{ref-glass}{\label{ref-glass}}
Glass, J., Hazen, T. J., Hetherington, L., \& Wang, C. (2004). Analysis
and processing of lecture audio data: Preliminary investigations. In
\emph{Proceedings of the workshop on interdisciplinary approaches to
speech indexing and retrieval at hLT-nAACL 2004} (pp. 9--12).
Association for Computational Linguistics.

\hyperdef{}{ref-hinton2012deep}{\label{ref-hinton2012deep}}
Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-r., Jaitly, N.,
Senior, A., et al. (2012). Deep neural networks for acoustic modeling in
speech recognition: The shared views of four research groups.
\emph{Signal Processing Magazine, IEEE}, \emph{29}(6), 82--97. IEEE.

\hyperdef{}{ref-kato2000}{\label{ref-kato2000}}
Kato, K., Nanjo, H., \& Kawahara, T. (2000). Automatic transcription of
lecture speech using topic-independent language modeling. In \emph{Sixth
international conference on spoken language processing}.

\hyperdef{}{ref-kawahara08}{\label{ref-kawahara08}}
Kawahara, T., Nemoto, Y., \& Akita, Y. (2008). Automatic lecture
transcription by exploiting presentation slide information for language
model adaptation. In \emph{Acoustics, speech and signal processing,
2008. iCASSP 2008. iEEE international conference on} (pp. 4929--4932).
IEEE.

\hyperdef{}{ref-bworld}{\label{ref-bworld}}
Lütkebohle, I. (2008). BWorld Robot Control Software.
\url{http://aiweb.techfak.uni-bielefeld.de/content/bworld-robot-control-software/}.

\hyperdef{}{ref-marquard}{\label{ref-marquard}}
Marquard, S. (2012). Improving searchability of automatically
transcribed lectures through dynamic language modelling. University of
Cape Town.

\hyperdef{}{ref-munteanu}{\label{ref-munteanu}}
Munteanu, C., Penn, G., \& Baecker, R. (2007). Web-based language
modelling for automatic lecture transcription. In \emph{INTERSPEECH}
(pp. 2353--2356).

\hyperdef{}{ref-openyale}{\label{ref-openyale}}
\emph{Open Yale Courses Website}. (n.d.). \url{http://oyc.yale.edu/}.

\hyperdef{}{ref-rabiner}{\label{ref-rabiner}}
Rabiner, L., \& Juang, B.-H. (1993). Fundamentals of speech recognition.
Prentice hall.

\hyperdef{}{ref-stevenson2011concise}{\label{ref-stevenson2011concise}}
Stevenson, A., \& Waite, M. (2011). \emph{Concise Oxford English
Dictionary: Book \& CD-ROM Set}. Oxford University Press.

\hyperdef{}{ref-yamazaki}{\label{ref-yamazaki}}
Yamazaki, H., Iwano, K., Shinoda, K., Furui, S., \& Yokota, H. (2007).
Dynamic language model adaptation using presentation slides for lecture
speech recognition. \emph{Proc. INTERSPEECH 2007}, 2349--2352.

\end{document}
